{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint \n",
    "#import seaborn as sns\n",
    "\n",
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whales to include based on what dataCSVs exist that dont exist\n",
    "# extracts the whale name name and video index (eg, Daffodil_1) based on the labeled videos that exist\n",
    "\n",
    "labeled_path = '../training_data/labeled_data/'\n",
    "fv_path = '../training_data/fv_norm_CSVs/'\n",
    "whale_names = list(os.walk(labeled_path))[0][2]\n",
    "whale_names = [n[:-4] for n in whale_names]\n",
    "\n",
    "CSV_names = [fv_path + n + '_images_fv_norm.csv' for n in whale_names]\n",
    "labeled_names = [labeled_path + n + '.csv' for n in whale_names]\n",
    "\n",
    "# verifies all corrosponding feature vector files exist\n",
    "existing = [fv_path + m for m in list(os.walk(fv_path))[0][2]]\n",
    "for n in CSV_names:\n",
    "    if n not in existing:\n",
    "        print(\"ALERT: file \", n, \" not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_path = '../training_data/labeled_data/frames/' #whale_video/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daffodil_1', 'Daffodil_2', 'Fan_1', 'Fan_2', 'Gom_1', 'Grommet_1']\n",
      "['../training_data/labeled_data/Daffodil_1.csv',\n",
      " '../training_data/labeled_data/Daffodil_2.csv',\n",
      " '../training_data/labeled_data/Fan_1.csv',\n",
      " '../training_data/labeled_data/Fan_2.csv',\n",
      " '../training_data/labeled_data/Gom_1.csv',\n",
      " '../training_data/labeled_data/Grommet_1.csv']\n",
      "['../training_data/fv_norm_CSVs/Daffodil_1_images_fv_norm.csv',\n",
      " '../training_data/fv_norm_CSVs/Daffodil_2_images_fv_norm.csv',\n",
      " '../training_data/fv_norm_CSVs/Fan_1_images_fv_norm.csv',\n",
      " '../training_data/fv_norm_CSVs/Fan_2_images_fv_norm.csv',\n",
      " '../training_data/fv_norm_CSVs/Gom_1_images_fv_norm.csv',\n",
      " '../training_data/fv_norm_CSVs/Grommet_1_images_fv_norm.csv']\n"
     ]
    }
   ],
   "source": [
    "pprint(whale_names)\n",
    "pprint(labeled_names)\n",
    "pprint(CSV_names)\n",
    "if VERBOSE: print(len(whale_names), len(labeled_names), len(CSV_names))\n",
    "if not (len(whale_names) == len(labeled_names) == len(CSV_names)):\n",
    "    print(\"ALERT: some error in files was detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daffodil_1\n",
      "Daffodil_2\n",
      "Fan_1\n",
      "Fan_2\n",
      "Gom_1\n",
      "Grommet_1\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "for labeled_name, CSV_name, w in zip(labeled_names, CSV_names, whale_names):  \n",
    "    xd = pd.read_csv(CSV_name, header = None)\n",
    "    yd = pd.read_csv(labeled_name, header = None)\n",
    "    print(w)\n",
    "    x_data.append(xd)\n",
    "    y_data.append(yd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERBOSE:\n",
    "    print(y_data[0:2])\n",
    "    print(x_data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_data is an array of dataframes\n",
    "#change this function to effect what the model trains to recognize\n",
    "def process_y(arr):\n",
    "    arr = [df.replace(to_replace='.*O.*', regex=True, value=1) for df in arr]\n",
    "    arr = [df.replace(to_replace='.*', regex=True, value=0) for df in arr]\n",
    "    return(arr)\n",
    "        \n",
    "def y_trim(arr):\n",
    "    imgs = [(df. iloc[:, 0]).tolist() for df in arr]\n",
    "    arr = [df.drop(axis='columns', labels=0) for df in arr]\n",
    "    return(imgs, arr)\n",
    "\n",
    "#y_data = [process_y(arr) for arr in y_data]\n",
    "imgs, y_data = y_trim(y_data)\n",
    "y_data = process_y(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERBOSE:\n",
    "    print(y_data[0:2])\n",
    "    print(y_data[0].max())\n",
    "    print(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_trim(arr):\n",
    "    arr = [df.drop(axis='columns', labels=0) for df in arr]\n",
    "    return(arr)\n",
    "\n",
    "x_data = x_trim(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERBOSE: print(x_data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for heldout video: Daffodil_1\n",
      "0.9679144385026738\n",
      "(340, 6) (340, 1)\n",
      "weight parameters:  [[-1.03178834 -0.90161759 -0.07092122  0.17553344  0.38228788  0.03208299]]\n",
      "[([0.8861519883338915, 0.11384801166610847],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00028000.jpg'),\n",
      " ([0.8877575390590138, 0.1122424609409862],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00027000.jpg'),\n",
      " ([0.8959233151339745, 0.10407668486602552],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00020000.jpg'),\n",
      " ([0.8967223125666772, 0.10327768743332277],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00025000.jpg'),\n",
      " ([0.901016747781695, 0.09898325221830498],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00026000.jpg'),\n",
      " ([0.907369159417859, 0.092630840582141],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00021000.jpg'),\n",
      " ([0.9109916623569299, 0.08900833764307013],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00002000.jpg'),\n",
      " ([0.9116992035797682, 0.08830079642023181],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00022000.jpg'),\n",
      " ([0.9155644344668653, 0.08443556553313468],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00003000.jpg'),\n",
      " ([0.9183566438591833, 0.0816433561408168],\n",
      "  '../whale_videos/Daffodil_1_images/Daffodil_1_00013000.jpg')]\n",
      "\n",
      "for heldout video: Daffodil_2\n",
      "0.9705882352941176\n",
      "(459, 6) (459, 1)\n",
      "weight parameters:  [[-1.29952117 -1.19533508  0.24113285  0.48459915  0.76962964  0.71304772]]\n",
      "[([0.7875434440046711, 0.2124565559953288],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00055000.jpg'),\n",
      " ([0.8023609971532789, 0.19763900284672106],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00037000.jpg'),\n",
      " ([0.8133344535320294, 0.18666554646797065],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00038000.jpg'),\n",
      " ([0.8633962197391657, 0.13660378026083428],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00064000.jpg'),\n",
      " ([0.8697061281532665, 0.13029387184673352],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00018000.jpg'),\n",
      " ([0.8750551320419245, 0.12494486795807543],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00015000.jpg'),\n",
      " ([0.8757771248412385, 0.1242228751587615],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00058000.jpg'),\n",
      " ([0.878082396909758, 0.12191760309024197],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00057000.jpg'),\n",
      " ([0.8801291515538674, 0.11987084844613263],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00056000.jpg'),\n",
      " ([0.8820394433880893, 0.11796055661191077],\n",
      "  '../whale_videos/Daffodil_2_images/Daffodil_2_00014000.jpg')]\n",
      "\n",
      "for heldout video: Fan_1\n",
      "0.9402985074626866\n",
      "(460, 6) (460, 1)\n",
      "weight parameters:  [[-1.49923266 -1.33663791  0.0984037   0.58296331  0.63445896  0.58131706]]\n",
      "[([0.6846003552912732, 0.3153996447087268],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00043000.jpg'),\n",
      " ([0.9326578808670929, 0.06734211913290715],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00042000.jpg'),\n",
      " ([0.9355274871013924, 0.06447251289860757],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00033000.jpg'),\n",
      " ([0.938057321898099, 0.06194267810190101],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00045000.jpg'),\n",
      " ([0.9404638852557381, 0.05953611474426189],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00041000.jpg'),\n",
      " ([0.9411979599482908, 0.05880204005170921],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00047000.jpg'),\n",
      " ([0.94181972987294, 0.05818027012705991],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00031000.jpg'),\n",
      " ([0.9474951569076081, 0.05250484309239185],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00032000.jpg'),\n",
      " ([0.9502683084576582, 0.04973169154234177],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00046000.jpg'),\n",
      " ([0.9542585861982286, 0.04574141380177147],\n",
      "  '../whale_videos/Fan_1_images/Fan_1_00030000.jpg')]\n",
      "\n",
      "for heldout video: Fan_2\n",
      "1.0\n",
      "(410, 6) (410, 1)\n",
      "weight parameters:  [[-1.09262709 -1.19054134 -0.08010141  0.4330734   0.85879854  0.84375483]]\n",
      "[([0.5857170757632258, 0.41428292423677415],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00071000.jpg'),\n",
      " ([0.7352194124537391, 0.2647805875462609],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00070000.jpg'),\n",
      " ([0.8202966269277199, 0.1797033730722801],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00073000.jpg'),\n",
      " ([0.820540057824715, 0.17945994217528505],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00072000.jpg'),\n",
      " ([0.8216650776474176, 0.1783349223525824],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00068000.jpg'),\n",
      " ([0.8353700069717132, 0.1646299930282869],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00069000.jpg'),\n",
      " ([0.9206465652609119, 0.07935343473908812],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00067000.jpg'),\n",
      " ([0.9366577185359647, 0.06334228146403531],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00064000.jpg'),\n",
      " ([0.944983259807349, 0.055016740192650966],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00066000.jpg'),\n",
      " ([0.9485161320020452, 0.05148386799795477],\n",
      "  '../whale_videos/Fan_2_images/Fan_2_00034000.jpg')]\n",
      "\n",
      "for heldout video: Gom_1\n",
      "0.9459459459459459\n",
      "(490, 6) (490, 1)\n",
      "weight parameters:  [[-1.35769746 -1.20952832  0.22319338  0.48733475  0.80243897  0.73979467]]\n",
      "[([0.9209252106305492, 0.07907478936945073],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00035000.jpg'),\n",
      " ([0.9246344976256621, 0.07536550237433788],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00027000.jpg'),\n",
      " ([0.9256408347467728, 0.07435916525322711],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00034000.jpg'),\n",
      " ([0.9259972663848874, 0.07400273361511257],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00033000.jpg'),\n",
      " ([0.9266587979749027, 0.07334120202509732],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00028000.jpg'),\n",
      " ([0.9273915235990661, 0.07260847640093392],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00008000.jpg'),\n",
      " ([0.9284208110881818, 0.07157918891181826],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00026000.jpg'),\n",
      " ([0.9285824789303765, 0.07141752106962346],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00006000.jpg'),\n",
      " ([0.9290158209649213, 0.07098417903507871],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00007000.jpg'),\n",
      " ([0.9295008543493756, 0.07049914565062444],\n",
      "  '../whale_videos/Gom_1_images/Gom_1_00032000.jpg')]\n",
      "\n",
      "for heldout video: Grommet_1\n",
      "0.9607843137254902\n",
      "(476, 6) (476, 1)\n",
      "weight parameters:  [[-1.35803876 -1.2893725   0.2356518   0.53509211  0.88205954  0.89897318]]\n",
      "[([0.7918737655438658, 0.2081262344561342],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00036000.jpg'),\n",
      " ([0.8044634706281077, 0.1955365293718923],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00037000.jpg'),\n",
      " ([0.8723839785024055, 0.1276160214975946],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00038000.jpg'),\n",
      " ([0.8891842897317767, 0.11081571026822334],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00042000.jpg'),\n",
      " ([0.894058730126838, 0.10594126987316196],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00043000.jpg'),\n",
      " ([0.9347290138915333, 0.06527098610846671],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00001000.jpg'),\n",
      " ([0.9366976901804275, 0.06330230981957256],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00008000.jpg'),\n",
      " ([0.939860248355509, 0.06013975164449099],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00041000.jpg'),\n",
      " ([0.9430171955793418, 0.05698280442065814],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00002000.jpg'),\n",
      " ([0.9450661716939044, 0.05493382830609565],\n",
      "  '../whale_videos/Grommet_1_images/Grommet_1_00003000.jpg')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cross validate, holding out exacrly one video for testing\n",
    "for i, name in enumerate(whale_names):\n",
    "    x_icl = x_data[0:i] + x_data[i+1:]\n",
    "    y_icl = y_data[0:i] + y_data[i+1:]\n",
    "    imgs_icl = imgs[0:i] + imgs[i+1:]\n",
    "    x_tr = np.concatenate(x_icl) #concatonate list of dataframes\n",
    "    y_tr = np.concatenate(y_icl)\n",
    "    imgs_tr = sum(imgs_icl, []) #concatonate list of lists\n",
    "    x_test = x_data[i]\n",
    "    y_test = y_data[i]\n",
    "    y_imgs = imgs[i]\n",
    "    clf = sklearn.linear_model.LogisticRegression(solver='liblinear').fit(x_tr, y_tr.T[0])\n",
    "    print(\"for heldout video: \" + name)\n",
    "    print(clf.score(x_test, y_test))\n",
    "    print(x_tr.shape, y_tr.shape)\n",
    "    print(\"weight parameters: \", clf.coef_)\n",
    "    probs = clf.predict_proba(x_test)\n",
    "    probs_list = probs.tolist()\n",
    "    pprint(sorted(list(zip(probs_list, y_imgs)))[:10])\n",
    "    print()\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "output_name = 'trained_model.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a final model on all labled data\n",
    "x_icl = x_data\n",
    "y_icl = y_data\n",
    "imgs_icl = imgs\n",
    "x_tr = np.concatenate(x_icl) #concatonate list of dataframes\n",
    "y_tr = np.concatenate(y_icl)\n",
    "imgs_tr = sum(imgs_icl, []) #concatonate list of lists\n",
    "clf = sklearn.linear_model.LogisticRegression(solver='liblinear').fit(x_tr, y_tr.T[0])\n",
    "file = open(output_name, \"wb\")\n",
    "pickle.dump(clf, file)\n",
    "#for importing, see https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
